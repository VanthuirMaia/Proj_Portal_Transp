# Portal da TransparÃªncia - Data Pipeline

[![Python](https://img.shields.io/badge/Python-3.12-3776AB?style=flat&logo=python&logoColor=white)](https://python.org)
[![dbt](https://img.shields.io/badge/dbt-1.11-FF694B?style=flat&logo=dbt&logoColor=white)](https://getdbt.com)
[![DuckDB](https://img.shields.io/badge/DuckDB-1.4-FEF000?style=flat&logo=duckdb&logoColor=black)](https://duckdb.org)
[![Streamlit](https://img.shields.io/badge/Streamlit-1.53-FF4B4B?style=flat&logo=streamlit&logoColor=white)](https://streamlit.io)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)

Pipeline completo de Engenharia de Dados que consome, transforma e analisa dados reais de despesas pÃºblicas do **Portal da TransparÃªncia do Governo Federal Brasileiro**.

![Pipeline Architecture](<https://img.shields.io/badge/Architecture-Medallion_(Bronzeâ†’Silverâ†’Gold)-blue?style=for-the-badge>)

---

## Sobre o Projeto

Este projeto implementa um **pipeline de dados end-to-end** utilizando dados reais da API do Portal da TransparÃªncia. O objetivo Ã© demonstrar habilidades prÃ¡ticas em Engenharia de Dados atravÃ©s de um caso de uso real, enfrentando desafios genuÃ­nos de qualidade, padronizaÃ§Ã£o e integraÃ§Ã£o de dados.

### Destaques

- **Dados Reais**: Consumo de APIs pÃºblicas com autenticaÃ§Ã£o e paginaÃ§Ã£o
- **Arquitetura Medallion**: Camadas Bronze (Raw), Silver (Staging) e Gold (Analytics)
- **Modelagem Dimensional**: Star Schema com dbt (dimensÃµes e fatos)
- **Qualidade de Dados**: ValidaÃ§Ãµes automatizadas em mÃºltiplas camadas
- **Dashboard Interativo**: VisualizaÃ§Ãµes em tempo real com Streamlit

---

## Arquitetura

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           PIPELINE DE DADOS                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   API REST   â”‚   Portal da TransparÃªncia
    â”‚   (Fonte)    â”‚   api.portaldatransparencia.gov.br
    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   1. INGESTÃƒO       â”‚   Python + Requests
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€     â”‚   â€¢ AutenticaÃ§Ã£o por API Key
â”‚   src/ingestion/    â”‚   â€¢ PaginaÃ§Ã£o automÃ¡tica
â”‚                     â”‚   â€¢ Rate limiting (0.3s)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   2. RAW (Bronze)   â”‚   CSV com timestamp
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€     â”‚   â€¢ Dados brutos
â”‚   data/raw/         â”‚   â€¢ Audit trail
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   3. STAGING        â”‚   Parquet + ValidaÃ§Ãµes
â”‚   (Silver)          â”‚   â€¢ Tipagem explÃ­cita
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€     â”‚   â€¢ NormalizaÃ§Ã£o monetÃ¡ria
â”‚   data/staging/     â”‚   â€¢ Quality checks
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   4. WAREHOUSE      â”‚   DuckDB
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€     â”‚   â€¢ OLAP columnar
â”‚   data/warehouse/   â”‚   â€¢ SQL analytics
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   5. TRANSFORM      â”‚   dbt + Star Schema
â”‚   (Gold)            â”‚   â”œâ”€â”€ stg_despesas_por_orgao
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€     â”‚   â”œâ”€â”€ dim_orgaos
â”‚   portal_transp_dbt â”‚   â””â”€â”€ fct_despesas
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   6. DASHBOARD      â”‚   Streamlit + Plotly
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€     â”‚   â€¢ KPIs interativos
â”‚   dashboard/        â”‚   â€¢ VisualizaÃ§Ãµes
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Stack TecnolÃ³gica

| Camada            | Tecnologia                  | PropÃ³sito                         |
| ----------------- | --------------------------- | --------------------------------- |
| **IngestÃ£o**      | Python, Requests            | Consumo de API REST com paginaÃ§Ã£o |
| **Armazenamento** | Parquet, DuckDB             | Formatos colunares otimizados     |
| **TransformaÃ§Ã£o** | dbt, Pandas                 | Modelagem dimensional e ETL       |
| **Qualidade**     | dbt tests, Python           | ValidaÃ§Ãµes e contratos de dados   |
| **OrquestraÃ§Ã£o**  | Apache Airflow 2.10.4       | OrquestraÃ§Ã£o de workflows         |
| **Containers**    | Docker & Docker Compose     | ContainerizaÃ§Ã£o                   |
| **VisualizaÃ§Ã£o**  | Streamlit, Plotly           | Dashboard interativo              |
| **Versionamento** | Git                         | Controle de versÃ£o                |

---

## Estrutura do Projeto

```
PROJ_PORTAL_TRANSP/
â”‚
â”œâ”€â”€ docker/
â”‚   â”œâ”€â”€ Dockerfile.ingestion
â”‚   â”œâ”€â”€ Dockerfile.transformation
â”‚   â”œâ”€â”€ Dockerfile.load
â”‚   â””â”€â”€ Dockerfile.dbt
â”‚
â”œâ”€â”€ airflow/
â”‚   â”œâ”€â”€ dags/
â”‚   â”œâ”€â”€ logs/
â”‚   â””â”€â”€ plugins/
â”‚
â”œâ”€â”€ docker-compose.yml
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                    # Bronze: CSVs brutos da API
â”‚   â”œâ”€â”€ staging/                # Silver: Parquet tratados
â”‚   â”œâ”€â”€ warehouse/              # DuckDB database
â”‚   â””â”€â”€ analytics/              # Gold: Dados agregados
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ ingestion/              # Scripts de ingestÃ£o
â”‚   â”‚   â”œâ”€â”€ fetch_orgaos_siafi.py
â”‚   â”‚   â””â”€â”€ fetch_despesas_por_orgao.py
â”‚   â”‚
â”‚   â”œâ”€â”€ transformation/         # ETL: Raw â†’ Staging
â”‚   â”‚   â””â”€â”€ stage_despesas_por_orgao.py
â”‚   â”‚
â”‚   â””â”€â”€ quality/                # ValidaÃ§Ãµes de qualidade
â”‚       â””â”€â”€ check_despesas_por_orgao.py
â”‚
â”œâ”€â”€ portal_transp_dbt/          # Projeto dbt
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ staging/
â”‚   â”‚   â”‚   â””â”€â”€ stg_despesas_por_orgao.sql
â”‚   â”‚   â””â”€â”€ marts/
â”‚   â”‚       â”œâ”€â”€ dim_orgaos.sql
â”‚   â”‚       â””â”€â”€ fct_despesas.sql
â”‚   â””â”€â”€ tests/
â”‚
â”œâ”€â”€ scripts/                    # UtilitÃ¡rios DuckDB
â”‚   â”œâ”€â”€ init_duckdb.py
â”‚   â””â”€â”€ load_staging_to_duckdb.py
â”‚
â”œâ”€â”€ dashboard/
â”‚   â””â”€â”€ app.py                  # Streamlit dashboard
â”‚
â”œâ”€â”€ run_pipeline.py             # Orquestrador principal
â”œâ”€â”€ requirements.txt
â””â”€â”€ .env.exemplo
```

---

## Modelo de Dados

### Star Schema

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   dim_orgaos    â”‚
                    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
                    â”‚ sk_orgao (PK)   â”‚
                    â”‚ codigo_orgao    â”‚
                    â”‚ nome_orgao      â”‚
                    â”‚ codigo_superior â”‚
                    â”‚ nome_superior   â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â”‚ 1:N
                             â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  fct_despesas   â”‚
                    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
                    â”‚ sk_orgao (FK)   â”‚
                    â”‚ ano             â”‚
                    â”‚ valor_empenhado â”‚
                    â”‚ valor_liquidado â”‚
                    â”‚ valor_pago      â”‚
                    â”‚ carga_timestamp â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### DicionÃ¡rio de Dados

| Campo             | Tipo    | DescriÃ§Ã£o                    |
| ----------------- | ------- | ---------------------------- |
| `ano`             | INTEGER | Ano fiscal (2020-2024)       |
| `codigo_orgao`    | VARCHAR | CÃ³digo SIAFI do Ã³rgÃ£o        |
| `orgao`           | VARCHAR | Nome do Ã³rgÃ£o                |
| `valor_empenhado` | DOUBLE  | Valor comprometido (R$)      |
| `valor_liquidado` | DOUBLE  | Valor verificado (R$)        |
| `valor_pago`      | DOUBLE  | Valor efetivamente pago (R$) |

---

## Qualidade de Dados

O pipeline implementa validaÃ§Ãµes em mÃºltiplas camadas:

### Regras de NegÃ³cio

| Regra                     | DescriÃ§Ã£o                             | Camada  |
| ------------------------- | ------------------------------------- | ------- |
| **Valores nÃ£o negativos** | Todos os campos monetÃ¡rios â‰¥ 0        | Staging |
| **CoerÃªncia financeira**  | empenhado â‰¥ liquidado â‰¥ pago          | Staging |
| **Unicidade lÃ³gica**      | Sem duplicatas em (ano, codigo_orgao) | Staging |

### Testes dbt

```yaml
# schema.yml
models:
  - name: stg_despesas_por_orgao
    columns:
      - name: ano
        tests: [not_null]
      - name: codigo_orgao
        tests: [not_null]
      - name: valor_empenhado
        tests: [not_null]

  - name: fct_despesas
    columns:
      - name: sk_orgao
        tests:
          - relationships:
              to: ref('dim_orgaos')
              field: sk_orgao
```

---

## Como Executar

### PrÃ©-requisitos

- Python 3.12+
- Chave de API do Portal da TransparÃªncia ([solicitar aqui](https://portaldatransparencia.gov.br/api-de-dados))

### InstalaÃ§Ã£o

```bash
# Clonar repositÃ³rio
git clone https://github.com/seu-usuario/portal-transparencia-pipeline.git
cd portal-transparencia-pipeline

# Criar ambiente virtual
python -m venv venv
source venv/bin/activate  # Linux/Mac
venv\Scripts\activate     # Windows

# Instalar dependÃªncias
pip install -r requirements.txt

# Configurar variÃ¡veis de ambiente
cp .env.exemplo .env
# Editar .env com sua API_KEY
```

### Executar Pipeline Completo

```bash
# Rodar todo o pipeline
python run_pipeline.py
```

### Executar Etapas Individuais

```bash
# 1. IngestÃ£o (API â†’ Raw)
python src/ingestion/fetch_despesas_por_orgao.py

# 2. TransformaÃ§Ã£o (Raw â†’ Staging)
python src/transformation/stage_despesas_por_orgao.py

# 3. Carregar no DuckDB
python scripts/load_staging_to_duckdb.py

# 4. Executar modelos dbt
cd portal_transp_dbt
dbt run
dbt test

# 5. Iniciar dashboard
streamlit run dashboard/app.py
```

---

## Dashboard

O dashboard interativo exibe:

- **KPIs**: Total empenhado, liquidado e pago
- **Top 10 Ã“rgÃ£os**: Ranking por valor pago
- **EvoluÃ§Ã£o Temporal**: SÃ©rie histÃ³rica 2020-2024
- **Filtros**: Por ano e Ã³rgÃ£o

```bash
# Iniciar dashboard
streamlit run dashboard/app.py
```

Acesse: `http://localhost:8501`

---

## API de Dados

### Endpoints Utilizados

| Endpoint                               | DescriÃ§Ã£o                    |
| -------------------------------------- | ---------------------------- |
| `GET /api-de-dados/orgaos-siafi`       | Lista de Ã³rgÃ£os SIAFI        |
| `GET /api-de-dados/despesas/por-orgao` | Despesas agregadas por Ã³rgÃ£o |

### AutenticaÃ§Ã£o

```python
headers = {
    "chave-api-dados": os.getenv("API_KEY"),
    "User-Agent": "data-engineering-study"
}
```

---

## Roadmap

| #  | Etapa                        | Status        |
|----|------------------------------|---------------|
| 1  | IngestÃ£o de dados via API    | âœ… ConcluÃ­do  |
| 2  | Camada Raw (Bronze)          | âœ… ConcluÃ­do  |
| 3  | Camada Staging (Silver)      | âœ… ConcluÃ­do  |
| 4  | ValidaÃ§Ãµes de qualidade      | âœ… ConcluÃ­do  |
| 5  | Warehouse DuckDB             | âœ… ConcluÃ­do  |
| 6  | Modelagem dimensional (dbt)  | âœ… ConcluÃ­do  |
| 7  | Testes automatizados         | âœ… ConcluÃ­do  |
| 8  | Dashboard Streamlit          | âœ… ConcluÃ­do  |
| 9  | CI/CD pipeline               | ğŸ”² Pendente   |
| 10 | OrquestraÃ§Ã£o com Airflow     | âœ… ConcluÃ­do  |
| 11 | DocumentaÃ§Ã£o dbt Docs        | ğŸ”² Pendente   |

---

## O que jÃ¡ foi implementado

### OrquestraÃ§Ã£o com Airflow (`airflow/`, `docker/`)
- Airflow configurado via Docker Compose (webserver + scheduler + postgres)
- Dockerfiles isolados para cada etapa do pipeline:
  - `Dockerfile.ingestion`: Container para ingestÃ£o via API
  - `Dockerfile.transformation`: Container para transformaÃ§Ã£o CSV â†’ Parquet
  - `Dockerfile.load`: Container para carga no DuckDB
  - `Dockerfile.dbt`: Container para modelagem e testes dbt
- DAG `portal_transparencia_pipeline` implementada com DockerOperator
- Cada task executa em container isolado (sem conflitos de dependÃªncias)
- Retry automÃ¡tico configurado (2 tentativas, 5 minutos de delay)
- Logs estruturados e rastreabilidade completa de execuÃ§Ãµes
- Schedule semanal (`@weekly`) com possibilidade de execuÃ§Ã£o manual
- Arquitetura production-ready e cross-platform

---

## PrÃ³ximos Passos

- [ ] Deploy do Airflow em ambiente produtivo (Kubernetes ou cloud managed)
- [ ] Implementar alertas via email/Slack em caso de falhas
- [ ] Expandir ingestÃ£o para outros Ã³rgÃ£os e categorias de despesas
- [ ] Adicionar testes de integraÃ§Ã£o end-to-end
- [ ] Criar dashboard executivo com mÃ©tricas de pipeline (SLA, data freshness)

---

## ğŸ³ Como Executar o Projeto

### PrÃ©-requisitos
- Docker Desktop instalado
- Git

### Executar Pipeline Completo com Airflow

1. Clone o repositÃ³rio
2. Navegue atÃ© a pasta do projeto
3. Suba os containers: `docker-compose up -d`
4. Acesse o Airflow: http://localhost:8080 (usuÃ¡rio: admin, senha: admin1234)
5. Ative e execute a DAG `portal_transparencia_pipeline`

### Visualizar Dashboard
```bash
cd dashboard
streamlit run app.py
```

Acesse: http://localhost:8501

### Parar os serviÃ§os
```bash
docker-compose down
```

---

## Aprendizados

Este projeto demonstra competÃªncias prÃ¡ticas em:

- **Data Engineering**: ConstruÃ§Ã£o de pipelines ETL/ELT robustos
- **Data Modeling**: Modelagem dimensional (Star Schema)
- **Data Quality**: ImplementaÃ§Ã£o de validaÃ§Ãµes e contratos
- **Modern Data Stack**: dbt, DuckDB, Streamlit
- **API Integration**: Consumo de APIs REST com autenticaÃ§Ã£o
- **Best Practices**: CÃ³digo limpo, versionamento, documentaÃ§Ã£o

---

## LicenÃ§a

Este projeto estÃ¡ sob a licenÃ§a MIT. Veja o arquivo [LICENSE](LICENSE) para mais detalhes.

---

## Autor

Desenvolvido como projeto de portfÃ³lio em **Engenharia de Dados**.

[![LinkedIn](https://img.shields.io/badge/LinkedIn-Connect-blue?style=flat&logo=linkedin)](https://www.linkedin.com/in/vanthuir-maia-47767810b/)
[![GitHub](https://img.shields.io/badge/GitHub-Follow-black?style=flat&logo=github)](https://github.com/VanthuirMaia)
[![Email](https://img.shields.io/badge/Email-vanmaiasf@gmail.com-red?style=flat&logo=gmail&logoColor=white)](mailto:vanmaiasf@gmail.com)
[![Email](https://img.shields.io/badge/Dev-vanthuir.dev@gmail.com-orange?style=flat&logo=gmail&logoColor=white)](mailto:vanthuir.dev@gmail.com)

---

<p align="center">
  <i>Dados reais. Problemas reais. SoluÃ§Ãµes profissionais.</i>
</p>
