# PROJ_PORTAL_TRANSP

**Projeto pr√°tico de Engenharia de Dados com dados p√∫blicos reais**

---

## üìå Vis√£o Geral

Este projeto foi criado com o objetivo de **aprender Engenharia de Dados de forma pr√°tica**, utilizando **dados reais do Portal da Transpar√™ncia do Governo Federal**, enfrentando problemas reais de qualidade, padroniza√ß√£o, volume e integra√ß√£o.

O foco n√£o √© apenas consumir dados, mas **construir um pipeline completo**, bem estruturado, version√°vel e evolutivo, seguindo boas pr√°ticas utilizadas em ambientes profissionais.

> Aprendizado baseado em **projeto**, n√£o em exemplos artificiais.

---

## üéØ Objetivos do Projeto

- Consumir APIs p√∫blicas reais com autentica√ß√£o e pagina√ß√£o
- Implementar camadas de dados (**RAW ‚Üí STAGING ‚Üí ANALYTICS**)
- Tratar dados inconsistentes e sem contrato
- Aplicar regras de **qualidade de dados**
- Evoluir para modelagem dimensional
- Utilizar **dbt** para transforma√ß√£o, testes e documenta√ß√£o
- Preparar o pipeline para orquestra√ß√£o com **Airflow**
- Consolidar o projeto como **case t√©cnico de portf√≥lio**

---

## üóÇÔ∏è Estrutura de Pastas

```
PROJ_PORTAL_TRANSP/
‚îÇ
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ raw/            # Dados brutos extra√≠dos da API (imut√°veis)
‚îÇ   ‚îú‚îÄ‚îÄ staging/        # Dados tratados e tipados (Silver Layer)
‚îÇ   ‚îî‚îÄ‚îÄ analytics/      # Dados prontos para an√°lise (Gold Layer)
‚îÇ
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ ingestion/      # Scripts de ingest√£o (API ‚Üí RAW)
‚îÇ   ‚îú‚îÄ‚îÄ transformation/ # Scripts de transforma√ß√£o (RAW ‚Üí STAGING)
‚îÇ   ‚îú‚îÄ‚îÄ quality/        # Regras e valida√ß√µes de qualidade de dados
‚îÇ   ‚îî‚îÄ‚îÄ utils/          # Fun√ß√µes utilit√°rias
‚îÇ
‚îú‚îÄ‚îÄ dbt/                # Projeto dbt (modelagem, testes, docs)
‚îú‚îÄ‚îÄ airflow/            # Orquestra√ß√£o (planejado)
‚îú‚îÄ‚îÄ notebooks/          # An√°lises explorat√≥rias e valida√ß√µes
‚îÇ
‚îú‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ .env.exemplo
‚îî‚îÄ‚îÄ README.md
```

---

## üîå Fonte de Dados

**Portal da Transpar√™ncia ‚Äì Governo Federal**

### Endpoint inicial (dimens√£o)

```
GET /api-de-dados/orgaos-siafi
```

### Endpoint principal (fato)

```
GET /api-de-dados/despesas/por-orgao
```

Os dados utilizados s√£o os mesmos disponibilizados publicamente no portal, acessados via API REST.

---

## ‚öôÔ∏è Tecnologias Utilizadas

- Python 3.12
- Pandas
- Requests
- PyArrow (Parquet)
- dotenv
- dbt (planejado)
- PostgreSQL / DuckDB (planejado)
- Airflow (planejado)

---

## üß≠ Roteiro do Projeto (checkpoint)

1. Escolher fonte de dados (API p√∫blica) ‚Äî **Conclu√≠do:** Portal da Transpar√™ncia (despesas/√≥rg√£os)
2. Ingest√£o Python: consumir API e salvar CSV local ‚Äî **Conclu√≠do:** scripts em `src/ingestion`, sa√≠das em `data/raw`
3. Camada staging: converter CSV ‚Üí Parquet em caminho separado ‚Äî **Conclu√≠do:** `src/transformation/stage_despesas_por_orgao.py` ‚Üí `data/staging`
4. dbt + PostgreSQL local: criar fatos/dimens√µes via SQL ‚Äî **Pendente**
5. Agrega√ß√µes SQL (GROUP BY, WINDOW) ‚Äî **Pendente** (planejado no dbt)
6. Airflow: orquestrar pipeline, notifica√ß√µes e retry ‚Äî **Pendente**
7. Visualiza√ß√£o: conectar Metabase/Power BI ao PostgreSQL e criar dashboard ‚Äî **Pendente**

Pr√≥ximos passos imediatos:

- Iniciar projeto dbt em `dbt/`, definir profile apontando para PostgreSQL local.
- Modelar staging e marts no dbt (fatos/dimens√µes) com testes declarativos.
- Carregar Parquet em PostgreSQL e validar agrega√ß√µes SQL.
- Preparar DAG no Airflow para orquestrar ingest√£o, staging, dbt e alertas.

---

## ‚úÖ O que j√° foi implementado

- Estrutura de pastas organizada e versionada
- Configura√ß√£o segura de vari√°veis de ambiente (`.env.exemplo`)
- Ingest√£o paginada de dados via API p√∫blica
- Camada **RAW** com versionamento por timestamp
- Transforma√ß√£o para **STAGING (Silver Layer)**:
  - Tipagem expl√≠cita
  - Normaliza√ß√£o de valores monet√°rios inconsistentes
  - Convers√£o para formato **Parquet**
- Cria√ß√£o de regras iniciais de **qualidade de dados**
- Valida√ß√£o de coer√™ncia financeira:
  - `empenhado ‚â• liquidado ‚â• pago`
- Separa√ß√£o clara entre **dimens√µes** e **fatos**

---

## üìä Dataset Atual (STAGING)

Tabela: `stg_despesas_por_orgao`

| Campo                 | Tipo      |
| --------------------- | --------- |
| ano                   | int       |
| codigo_orgao          | string    |
| orgao                 | string    |
| codigo_orgao_superior | string    |
| orgao_superior        | string    |
| valor_empenhado       | float     |
| valor_liquidado       | float     |
| valor_pago            | float     |
| carga_timestamp       | timestamp |

---

## üß™ Qualidade de Dados

Regras implementadas:

- Valores monet√°rios n√£o negativos
- Coer√™ncia financeira entre empenhado, liquidado e pago
- Unicidade l√≥gica por `(ano, codigo_orgao)`

Essas regras servem como base para contratos de dados e testes futuros no dbt.

---

## üöÄ Pr√≥ximos Passos

#### Fase 1 ‚Äî Consolida√ß√£o t√©cnica

- Consolidar regras de qualidade como contratos formais
- Criar projeto dbt (adapter DuckDB)
- Modelar camadas staging e marts
- Implementar testes declarativos no dbt
- Gerar documenta√ß√£o autom√°tica dos modelos

#### Fase 2 ‚Äî An√°lise e consumo

- Estruturar o banco anal√≠tico como camada de consumo
- Criar consultas SQL anal√≠ticas (agrega√ß√µes, rankings, m√©tricas)
- Explorar visualiza√ß√µes (Power BI / Metabase)

#### Fase 3 ‚Äî Automa√ß√£o e maturidade

- Orquestrar o pipeline com Airflow
- Transformar o projeto em case de portf√≥lio t√©cnico

---

## üìå Filosofia do Projeto

Este projeto segue uma abordagem **realista**:

- Dados reais s√£o imperfeitos
- APIs falham
- Ambientes quebram
- Qualidade precisa ser expl√≠cita
- Engenharia vem antes de dashboards

> O objetivo n√£o √© apenas fazer funcionar,  
> √© **entender, justificar e sustentar cada decis√£o t√©cnica**.

---

## ‚úçÔ∏è Autor

Projeto desenvolvido para estudo e aprofundamento em **Engenharia de Dados**, com foco em aprendizado cont√≠nuo baseado em projetos reais.
